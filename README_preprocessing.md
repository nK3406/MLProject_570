# README â€“ Veri Ã–n Ä°ÅŸleme (Preprocessing) Pipeline

Bu belge, `MLProject_570` deposunda ham veriden **LSTM modeline** beslenmeye hazÄ±r
nÃ¼merik tensÃ¶rlere kadar takip edilen tÃ¼m veri Ã¶n iÅŸleme (preprocessing)
adÄ±mlarÄ±nÄ± ayrÄ±ntÄ±lÄ± olarak aÃ§Ä±klar. AmaÃ§, sÃ¼reci yeniden Ã§alÄ±ÅŸtÄ±rmak veya
kendi verinize uygulamak isteyen araÅŸtÄ±rmacÄ±lara **uÃ§tan uca yol haritasÄ±**
sunmaktÄ±r.

> ğŸ“ **KÄ±sa Ã–zet**  
> 1. Ham kaynak dosyalarÄ±nÄ± iÃ§e aktarÄ±n (LargeST *HDF5* & SWITRS *SQLite*).  
> 2. Notebookâ€™larla biÃ§im dÃ¶nÃ¼ÅŸÃ¼mlerini yapÄ±n (HDF5â†’Parquet, SQLiteâ†’CSV).  
> 3. `src/utils/lstm_prep.py` ile Parquet + meta veriyi LSTM dizilerine Ã§evirin.  
> 4. Ã‡Ä±ktÄ±larÄ± `data/processed/` ve `preprocessed_data/` altÄ±nda bulun.

---

## 1 | Kurulum

```bash
# Sanal ortam (Ã¶nerilir)
python -m venv .venv && source .venv/bin/activate

# Gerekli paketler
pip install -r requirements.txt
```

Python â‰¥ 3.9, Dask ve Scikit-learn uyumlu bir ortam gereklidir.

---

## 2 | Hammadde KaynaklarÄ±

| Kaynak | BiÃ§im | Yol / URL | Ä°Ã§erik |
| ------ | ----- | --------- | ------ |
| **LargeST** | `.h5` | `data/raw/largest.h5` | Kaliforniya 5-dk trafik yoÄŸunluk sensÃ¶r kayÄ±tlarÄ± |
| **SWITRS** | `.sqlite` | `data/raw/SWITRS.sqlite` | Kaza veritabanÄ± (2008-2019) |
| **Meta** | `.csv` | `data/raw/ca_meta.csv` | SensÃ¶r konum & Ã¶zellikler |

---

## 3 | Notebook TabanlÄ± DÃ¶nÃ¼ÅŸÃ¼mler

### 3.1 `00_inspect_largest_h5.ipynb`
* **AmaÃ§:** HDF5 hiyerarÅŸisini keÅŸfetmek, uygun sensÃ¶r ve
yÄ±l aralÄ±klarÄ±nÄ± belirlemek.
* **Ã‡Ä±ktÄ±:** SeÃ§ilen dÃ¼ÄŸÃ¼mlerin listesi (`largest_selected.json`).

### 3.2 `01_prep_largest_to_parquet.ipynb`
* **Girdi:** `largest.h5` & seÃ§im listesi.
* **Ä°ÅŸlem AdÄ±mlarÄ±**
  1. HDF5 tabakalarÄ±nÄ± okunabilir parÃ§alara bÃ¶ler.
  2. Her parÃ§ayÄ± Dask ile DataFrameâ€™e aktarÄ±r.
  3. Blok boyutu = 100 MB olacak ÅŸekilde **Parquet** dosyalarÄ±na yazar.
* **Ã‡Ä±ktÄ±:** `data/processed/largest_wide_2017/` (partitioned Parquet).

### 3.3 `02_prep_switrs_to_csv.ipynb`
* **Girdi:** `SWITRS.sqlite`.
* **Ä°ÅŸlem AdÄ±mlarÄ±**
  1. `collisions` tablosunu SQLiteâ€™dan Ã§ek.
  2. Gereksiz sÃ¼tunlarÄ± dÃ¼ÅŸÃ¼r; eksik kodlamalarÄ± dÃ¼zelt.
  3. Son DataFrameâ€™i **CSV** olarak kaydet.
* **Ã‡Ä±ktÄ±:** `data/processed/collisions_2017.csv`.

> **Not:** Notebookâ€™lar gÃ¶rsel inceleme ve ara adÄ±m doÄŸrulamasÄ± iÃ§in
> interaktiftir; tam otomasyon tercih ediyorsanÄ±z aynÄ± kodu bir Python
> betiÄŸine taÅŸÄ±yabilirsiniz.

---

## 4 | LSTMâ€™e HazÄ±rlÄ±k â€“ `src/utils/lstm_prep.py`
AÅŸaÄŸÄ±daki fonksiyonlar boru hattÄ±nÄ±n kodlanmÄ±ÅŸ halidir. Komut satÄ±rÄ±ndan
veya baÅŸka bir modÃ¼lden Ã§aÄŸrÄ±labilir.

| AdÄ±m | Fonksiyon | AÃ§Ä±klama |
| ---- | --------- | -------- |
| 1 | `load_data` | Parquet (bÃ¼yÃ¼k) + meta CSVâ€™yi Dask & Pandas ile yÃ¼kler. |
| 2 | `impute_missing` | %50â€™den fazla eksik veriye sahip sensÃ¶rleri eler, kalan
  deÄŸerleri **lineer enterpolasyon** ile doldurur. |
| 3 | `preprocess_timestamps` | Zaman damgalarÄ±nÄ± `datetime64[ns]`â€™e Ã§evirir,
  5-dakikalÄ±k aralÄ±klarla **yeniden Ã¶rnekler** ve enterpole eder. |
| 4 | `merge_metadata` | SensÃ¶r-ID ile meta veriyi birleÅŸtirir; sayÄ±sallar
  Min-Max Ã¶lÃ§eklenir, kategorikler **One-Hot** kodlanÄ±r. |
| 5 | `create_sequences` | Her sensÃ¶r iÃ§in [seq_length] uzunlukta kayan pencereler
  oluÅŸturur; meta Ã¶zellikleri her time-stepâ€™e ekler. |
| 6 | `split_data` | Dizileri **train / val / test** oranlarÄ±na gÃ¶re bÃ¶ler. |
| 7 | `save_sequences` | Numpy `.npy` olarak diske yazar (Ã¶rn. `train_sequences.npy`). |

Ã–ntanÄ±mlÄ± parametreler:
```python
train_ratio = 0.70
val_ratio   = 0.20  # Test = 0.10
seq_length  = 12    # 12 Ã— 5-dk = 1 saatlik pencere
overlap     = 12    # %100 Ã¶rtÃ¼ÅŸme
```

Ã‡alÄ±ÅŸtÄ±rma Ã¶rneÄŸi:
```bash
python -m src.utils.lstm_prep \
  --parquet_path data/processed/largest_wide_2017 \
  --meta_path    data/raw/ca_meta.csv \
  --seq_length 12 --overlap 12
```

---

## 5 | Ã‡Ä±ktÄ± Dizinleri

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ largest_wide_2017/   # Partitioned Parquet
â”‚       â””â”€â”€ collisions_2017.csv
â””â”€â”€ preprocessed_data/
    â”œâ”€â”€ train_sequences.npy
    â”œâ”€â”€ val_sequences.npy
    â”œâ”€â”€ test_sequences.npy
    â”œâ”€â”€ train_targets.npy
    â”œâ”€â”€ ...
    â””â”€â”€ processed_meta.csv
```

Bu Ã§Ä±ktÄ±lar doÄŸrudan *model eÄŸitimi* aÅŸamasÄ±nda kullanÄ±lmaya hazÄ±rdÄ±r.

---

## 6 | Ä°puÃ§larÄ± & Sorun Giderme

* **Bellek HatasÄ±:** `dd.read_parquet(..., blocksize="50MB")` parametresiyle
  boyutlarÄ± kÃ¼Ã§Ã¼ltÃ¼n.
* **Tarih UyumsuzluÄŸu:** HDF5 sensÃ¶r zaman aralÄ±klarÄ± yÄ±l bazÄ±nda deÄŸiÅŸebilir;
  notebookâ€™ta filtreleme yaptÄ±ÄŸÄ±nÄ±zdan emin olun.
* **Meta-Veri Eksik:** `ca_meta.csv` iÃ§inde sensÃ¶r ID eÅŸleÅŸmiyorsa ilgili
  sensÃ¶rÃ¼ **impute_missing** adÄ±mÄ±nda zaten elenmiÅŸ olabilir.

---

ğŸ‰ ArtÄ±k veriniz LSTM tabanlÄ± trafik yoÄŸunluk tahmini iÃ§in hazÄ±r!
