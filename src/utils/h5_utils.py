"""
LargeST HDF5 dosyalarÄ±nÄ± inceleme ve Parquetâ€™e dÃ¶nÃ¼ÅŸtÃ¼rme yardÄ±mcÄ± iÅŸlevleri.
"""
import h5py
import pandas as pd
from pathlib import Path
from tqdm import tqdm

def list_h5_structure(h5_path: Path, max_depth: int = 4) -> None:
    """
    HDF5 dosyasÄ±ndaki grup/dataset yapÄ±sÄ±nÄ± hiyerarÅŸik olarak yazdÄ±rÄ±r.
    Dataset dÃ¼ÄŸÃ¼mlerinde .items() Ã§aÄŸrÄ±sÄ± yapÄ±lmaz.
    """

    def _traverse(name, obj, depth):
        indent = "  " * depth
        if isinstance(obj, h5py.Dataset):
            # Dataset: sadece ad, shape ve dtype gÃ¶ster
            print(f"{indent}ğŸ“„ {name}  {obj.shape}  {obj.dtype}")
        elif isinstance(obj, h5py.Group):
            # Group: adÄ± yaz, sonra iÃ§indekileri gez
            print(f"{indent}ğŸ“ {name}/")
            if depth < max_depth:
                for key, val in obj.items():
                    child_name = f"{name}/{key}" if name else key
                    _traverse(child_name, val, depth + 1)
        else:
            # DiÄŸer hdf5 objeleri (nadir)
            print(f"{indent}â” {name}")

    h5_path = Path(h5_path)
    if not h5_path.exists():
        raise FileNotFoundError(h5_path)

    with h5py.File(h5_path, "r") as f:
        _traverse("", f, 0)

def h5_to_parquet(
    h5_path: Path,
    parquet_path: Path,
    dataset_key: str,
    timestamp_key: str = "t/axis1",
    columns: list = None,
    chunk_rows: int = None,
) -> None:
    """
    Belirtilen bir HDF5 dataset'ini pandas DataFrame'e Ã§evirip, zaman damgasÄ±nÄ± (timestamp) ilk sÃ¼tuna ekleyerek Parquet olarak kaydeder.

    Args:
        h5_path (Path): HDF5 dosya yolu
        parquet_path (Path): Parquet dosya yolu (veya klasÃ¶rÃ¼)
        dataset_key (str): HDF5 iÃ§indeki dataset anahtarÄ± (Ã¶rn: 'mygroup/mydataset')
        timestamp_key (str): Zaman damgasÄ± dataset anahtarÄ± (Ã¶rn: 't/axis1'). DataFrame'in ilk sÃ¼tunu olarak eklenir.
        columns (list, optional): DataFrame sÃ¼tun isimleri. None ise varsayÄ±lan isimler kullanÄ±lÄ±r.
        chunk_rows (int, optional): SatÄ±r bazÄ±nda parÃ§a parÃ§a okuma (bÃ¼yÃ¼k dosyalar iÃ§in). None ise tamamÄ± okunur.
    """
    h5_path = Path(h5_path)
    parquet_path = Path(parquet_path)
    with h5py.File(h5_path, "r") as f:
        ds = f[dataset_key]
        timestamps = f[timestamp_key][:]
        n_rows = ds.shape[0]
        n_cols = ds.shape[1] if len(ds.shape) > 1 else 1
        if chunk_rows is None:
            data = ds[:]
            df = pd.DataFrame(data, columns=columns)
            df.index = pd.Index(timestamps, name="timestamp")
            df.to_parquet(parquet_path, index=True)
        else:
            parquet_path.parent.mkdir(parents=True, exist_ok=True)
            for start in tqdm(range(0, n_rows, chunk_rows), desc=f"{h5_path.name} chunks"):
                stop = min(start + chunk_rows, n_rows)
                data = ds[start:stop]
                df = pd.DataFrame(data, columns=columns)
                df.index = pd.Index(timestamps[start:stop], name="timestamp")
                out_file = parquet_path.parent / f"{h5_path.stem}_{start}_{stop-1}.parquet"
                df.to_parquet(out_file, index=True)


def h5_block_to_parquet_wide(
    h5_path: Path,
    out_dir: Path,
    block_key: str = "t/block0_values",
    axis0_key: str = "t/axis0",
    axis1_key: str = "t/axis1",
    chunk_rows: int = 24 * 60,        # 1 gÃ¼n = 1440 satÄ±r
    add_timestamp: bool = True,
) -> None:
    """
    LargeST HDF5 dosyasÄ±ndaki 'block0_values' (zaman Ã— sensÃ¶r) matrisini
    geniÅŸ formatta (sensor_id'ler sÃ¼tun) Parquet dosyalarÄ±na yazar.

    - Her 'chunk_rows' kadar satÄ±r bir Parquet dosyasÄ±na gider.
    - Bellek verimliliÄŸi iÃ§in blok blok okur.
    """
    h5_path = Path(h5_path)
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    with h5py.File(h5_path, "r") as f:
        values_ds  = f[block_key]
        sensor_ids = f[axis0_key][:].astype(str)      # (8600,)
        timestamps = f[axis1_key][:]                  # (N,)

        n_rows = values_ds.shape[0]

        for start in tqdm(range(0, n_rows, chunk_rows), desc=h5_path.name):
            stop = min(start + chunk_rows, n_rows)

            # --- Chunkâ€™Ä± oku ---
            val_chunk = values_ds[start:stop, :]        # ndarray  (chunk_rows Ã— 8600)

            # --- DataFrame oluÅŸtur ---
            df = pd.DataFrame(val_chunk, columns=sensor_ids)

            if add_timestamp:
                df.insert(0, "timestamp", timestamps[start:stop])

            # --- Parquet kaydet ---
            out_file = out_dir / f"{h5_path.stem}_{start}_{stop-1}.parquet"
            df.to_parquet(out_file, index=False)